{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51aea38f-54a3-4bcb-a31a-86d2d9d54ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")  # access to local modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed42b330-8adf-49c9-a3f0-e6363c9f29f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-02 14:12:48.536599: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.17.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import partial\n",
    "from sklearn.datasets import load_sample_images\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8192a321-3319-4219-b0d4-63ceec71bd99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-02 14:12:50.081252: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:920] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-04-02 14:12:52.199108: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:920] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-04-02 14:12:52.199152: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:920] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-04-02 14:12:52.199162: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2397] Ignoring visible gpu device (device: 0, name: AMD Radeon Graphics, pci bus id: 0000:65:00.0) with AMDGPU version : gfx1103. The supported AMDGPU versions are gfx900, gfx906, gfx908, gfx90a, gfx940, gfx941, gfx942, gfx1030, gfx1100, gfx1101, gfx1102, gfx1200, gfx1201.\n"
     ]
    }
   ],
   "source": [
    "images = load_sample_images()[\"images\"]\n",
    "images = tf.keras.layers.CenterCrop(height=70, width=120)(images)\n",
    "images = tf.keras.layers.Rescaling(scale  = 1 / 255)(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e10f550-6b8f-4bb5-b23d-621a23fd1ce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 70, 120, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7d98fea-8343-4b29-b3c4-05db366dea64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 64, 114, 32])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_layer = tf.keras.layers.Conv2D(filters=32, kernel_size=7)\n",
    "fmaps = conv_layer(images)\n",
    "fmaps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e6a4dd9-9351-4137-a128-c10f0f63ebb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 70, 120, 32])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_layer = tf.keras.layers.Conv2D(filters=32, kernel_size=7, padding=\"same\")\n",
    "fmaps = conv_layer(images)\n",
    "fmaps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b44a30d9-fa3a-4f8d-9f06-1784dcd27ee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 7, 3, 32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernels, biases = conv_layer.get_weights()\n",
    "kernels.shape  # [height, width, input channels, output channels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df7de15c-5a2d-4d34-b005-3aff993df690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases.shape  # output channels only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cd74368-fe3c-411e-b217-9eca5876e9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pool = tf.keras.layers.MaxPool2D(pool_size=2)\n",
    "avg_pool = tf.keras.layers.AvgPool2D(pool_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7286c30-5f10-486b-9a4b-7f0e5dd076e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthPool(tf.keras.layers.Layer):\n",
    "    def __init__(self, pool_size=2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self._pool_size = pool_size\n",
    "\n",
    "    def call(self, inputs):\n",
    "        shape = tf.shape(inputs)\n",
    "        groups = shape[-1] // self.pool_size  # number of channel groups\n",
    "        new_shape = tf.concat([shape[:-1], [groups, self._pool_size]], axis=0)\n",
    "        return tf.reduce_max(tf.reshape(inputs, new_shape), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65f73c18-9dd1-4c01-8293-9720e418e250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[0.64338624, 0.5971759 , 0.5824972 ],\n",
       "       [0.76306933, 0.2601113 , 0.10849128]], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_avg_pool = tf.keras.layers.GlobalAvgPool2D()  # outputs the means of each feature map\n",
    "global_avg_pool(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6648e85f-9ee6-4804-b59d-c2ac1d77c212",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dierck/workspace/python/MLTraining/Notebooks/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "DefaultConv2D = partial(tf.keras.layers.Conv2D, kernel_size=3, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\")\n",
    "model = tf.keras.Sequential([  # in practical nets, use a reshape layer here first\n",
    "    DefaultConv2D(filters=64, kernel_size=7, input_shape=[28, 28, 1]),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    DefaultConv2D(filters=128),\n",
    "    DefaultConv2D(filters=128),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    DefaultConv2D(filters=256),\n",
    "    DefaultConv2D(filters=256),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(units=128, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(units=64, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(units=10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9315d6d-e3f3-428a-90c0-527cb43b2b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.fashion_mnist.load_data()\n",
    "(X_train_full, y_train_full), (X_test, y_test) = mnist\n",
    "X_train_full = np.expand_dims(X_train_full, axis=-1).astype(np.float32) / 255\n",
    "X_test = np.expand_dims(X_test.astype(np.float32), axis=-1) / 255\n",
    "X_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]\n",
    "y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53d0dea5-4169-4149-b771-4132b4280253",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   takes about 30 minutes to train\n",
    "\n",
    "# model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "# history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34a8544d-adf0-4ff0-a24c-5c8ecc7fe465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# score = model.evaluate(X_test, y_test)\n",
    "# X_new = X_test[:10]  # pretend we have new images\n",
    "# y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47751e5f-d134-41fd-9cb6-3c1fae8e7d98",
   "metadata": {},
   "source": [
    "# ResNet-34 from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80363c23-0c90-4c0d-91f9-2ef538501211",
   "metadata": {},
   "outputs": [],
   "source": [
    "DefaultConv2D = partial(tf.keras.layers.Conv2D, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False)\n",
    "\n",
    "class ResidualUnit(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters, strides=1, activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self._activation = tf.keras.activations.get(activation)\n",
    "        self._main_layers = [\n",
    "            DefaultConv2D(filters, strides=strides),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            self._activation,\n",
    "            DefaultConv2D(filters),\n",
    "            tf.keras.layers.BatchNormalization()\n",
    "        ]\n",
    "        self._skip_layers = []\n",
    "        if strides > 1:\n",
    "            self._skip_layers = [\n",
    "                DefaultConv2D(filters, kernel_size=1, strides=strides),\n",
    "                tf.keras.layers.BatchNormalization()\n",
    "            ]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self._main_layers:\n",
    "            Z = layer(Z)\n",
    "        skip_Z = inputs\n",
    "        for layer in self._skip_layers:\n",
    "            skip_Z = layer(skip_Z)\n",
    "\n",
    "        return self._activation(Z + skip_Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0c4a34e-667c-4f64-837b-beb0517c6d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([DefaultConv2D(64, kernel_size=7, strides=2, input_shape=[224, 224, 3]),\n",
    "                             tf.keras.layers.BatchNormalization(),\n",
    "                             tf.keras.layers.Activation(\"relu\"),\n",
    "                             tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding=\"same\")\n",
    "                            ])\n",
    "prev_filters = 64\n",
    "for filters in [64] * 3 + [128] * 4 + [256] * 6 + [512] * 3:\n",
    "    strides = 1 if filters == prev_filters else 2\n",
    "    model.add(ResidualUnit(filters, strides=strides))\n",
    "    prev_filters = filters\n",
    "\n",
    "model.add(tf.keras.layers.GlobalAvgPool2D())\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91433525-5580-4e61-85c2-5193d028b013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "K = tf.keras.backend\n",
    "images = K.constant(load_sample_images()[\"images\"])\n",
    "print(len(images))\n",
    "images_resized = tf.keras.layers.Resizing(height=224, width=224, crop_to_aspect_ratio=True)(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4bc3abda-0b9a-4c05-bf56-7ca82520e653",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.applications.resnet50.preprocess_input(images_resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0437e0c8-714f-41fb-b080-1f4aae19e07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.applications.ResNet50(weights=\"imagenet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f94b22da-f64c-4032-a064-2287f81d63cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 790ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 1000)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_proba = model.predict(inputs)\n",
    "Y_proba.shape  # (number of images, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa4add68-e2a9-43ab-ac52-573107814feb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21528da9-6ff2-4a84-8be6-9a6e6cf6a785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image #0\n",
      "\tn03877845 - palace       54.693586%\n",
      "\tn03781244 - monastery    24.714711%\n",
      "\tn02825657 - bell_cote    18.550421%\n",
      "Image #1\n",
      "\tn04522168 - vase         32.669640%\n",
      "\tn11939491 - daisy        17.817330%\n",
      "\tn03530642 - honeycomb    12.041561%\n"
     ]
    }
   ],
   "source": [
    "top_K = tf.keras.applications.resnet50.decode_predictions(Y_proba, top=3)\n",
    "for image_index in range(len(images)):\n",
    "    print(f\"Image #{image_index}\")\n",
    "    for class_id, name, y_proba in top_K[image_index]:\n",
    "        print(f\"\\t{class_id} - {name:12s} {y_proba:-2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b614337e-a4be-4c54-bab4-f081ab491482",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8474634e-e7cc-490c-b504-134dd4c5a57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, info = tfds.load(\"tf_flowers\", as_supervised=True, with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb12ec51-d289-45ce-8fa8-93ca24d88d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "datset_size = info.splits[\"train\"].num_examples\n",
    "class_names = info.features[\"label\"].names\n",
    "n_classes = info.features[\"label\"].num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2695a2d3-504e-4801-99ce-5553ba200596",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_raw, valid_set_raw, train_set_raw = tfds.load(\"tf_flowers\", split=[\"train[:10%]\", \"train[10%:25%]\", \"train[25%:]\"], as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b8032449-b0c3-4b96-8404-e8e283173d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "preprocess = tf.keras.Sequential([\n",
    "    tf.keras.layers.Resizing(height=224, width=224, crop_to_aspect_ratio=True),\n",
    "    tf.keras.layers.Lambda(tf.keras.applications.xception.preprocess_input)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "addd816c-90fc-4197-84bb-34d49b7aa6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_set_raw.map(lambda X, y: (preprocess(X), y))\n",
    "train_set = train_set.shuffle(1000, seed=42).batch(batch_size).prefetch(1)\n",
    "valid_set = valid_set_raw.map(lambda X, y: (preprocess(X), y)).batch(batch_size)\n",
    "test_set = test_set_raw.map(lambda X, y: (preprocess(X), y)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3f9dc807-009d-4008-aea0-9b6cfd6d3b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(mode=\"horizontal\", seed=42),\n",
    "    tf.keras.layers.RandomRotation(factor=0.05, seed=42),\n",
    "    tf.keras.layers.RandomContrast(factor=0.2, seed=42)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "68d3dafe-0389-4a86-9783-c02dd867733c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.xception.Xception(weights=\"imagenet\", include_top=False)\n",
    "avg = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "output = tf.keras.layers.Dense(n_classes, activation=\"softmax\")(avg)\n",
    "model = tf.keras.Model(inputs=base_model.input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e55dcf90-9998-416e-8665-04c18e84b65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c6b6d764-d9aa-4067-a2ec-28ffc36db232",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "# history = model.fit(train_set, validation_data=valid_set, epochs=3)   # takes about 8 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "24c1ba2c-263a-44bd-9914-e26e2daf33d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers[56:]:  # unfreeze residual unit 7 out of 14 and above\n",
    "    layer.trainable = True\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "# history = model.fit(train_set, validation_data=valid_set, epochs=10)  #  takes about 40 minutes..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fba736e-7d65-4ca7-8585-ca48ec636a84",
   "metadata": {},
   "source": [
    "# Localization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5f973a57-63e0-4c65-b235-e31b922c6292",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.xception.Xception(weights=\"imagenet\", include_top=False)\n",
    "avg = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "class_output = tf.keras.layers.Dense(n_classes, activation=\"softmax\")(avg)\n",
    "loc_output = tf.keras.layers.Dense(4)(avg)\n",
    "model = tf.keras.Model(inputs=base_model.input, outputs = [class_output, loc_output])\n",
    "model.compile(loss=[\"sparse_categorical_crossentropy\", \"mse\"], loss_weights = [0.8, 0.2], optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "# missing labels, and training would take very long time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf441322-98b1-46c6-a192-4ef1ab8cd4c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
